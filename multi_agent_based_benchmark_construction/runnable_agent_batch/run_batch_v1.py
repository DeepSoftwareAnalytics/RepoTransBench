# file path: runnable_agent_batch/run_batch_v1.py

"""
å¤šè¿›ç¨‹æ‰¹é‡è¿è¡Œè„šæœ¬ - ä¸“é—¨å¤„ç†Matlabé¡¹ç›®
å°†1885ä¸ªMatlabé¡¹ç›®åˆ†æˆ10ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†
"""

import argparse
import logging
import os
import sys
import time
import traceback
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import json
import tiktoken
import multiprocessing
from multiprocessing import Pool, Manager, Process
from runnable_agent_batch.run import TestDetectionPipeline


def setup_logging(process_id: int = 0):
    """ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—"""
    log_format = f'%(asctime)s - Process-{process_id} - %(levelname)s - %(name)s - %(message)s'
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    logger = logging.getLogger()
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # è®¾ç½®æ–°çš„å¤„ç†å™¨
    logging.basicConfig(
        format=log_format,
        datefmt='%m/%d/%Y %H:%M:%S',
        level=logging.INFO,
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(f'multiprocess_batch_debug_p{process_id}.log')
        ]
    )
    return logging.getLogger(__name__)


def run_single_project(args_tuple: Tuple) -> Dict:
    """è¿è¡Œå•ä¸ªé¡¹ç›®çš„æµ‹è¯•æ£€æµ‹ç®¡é“ - ç”¨äºå¤šè¿›ç¨‹"""
    project_path, language, model_name, max_iterations, timeout_per_command, verbose, process_id, project_index, total_projects = args_tuple
    
    # ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—
    logger = setup_logging(process_id)
    
    project_name = project_path.name
    start_time = datetime.now()
    
    logger.info(f"ğŸ” [{project_index}/{total_projects}] Starting test detection for: {project_name}")
    
    result = {
        'process_id': process_id,
        'project_index': project_index,
        'project_name': project_name,
        'language': language,
        'project_path': str(project_path),
        'start_time': start_time.isoformat(),
        'status': 'unknown',
        'error_message': None,
        'iterations_completed': 0,
        'execution_time_seconds': 0,
        'copied_to_runnable': False
    }
    
    try:
        # åˆ›å»ºTestDetectionPipelineçš„å‚æ•°
        pipeline_args = argparse.Namespace(
            model_name=model_name,
            repo_path=str(project_path),
            repo_language=language,
            max_iterations=max_iterations,
            timeout_per_command=timeout_per_command,
            verbose=verbose
        )
        
        # åˆå§‹åŒ–å¹¶è¿è¡Œç®¡é“
        pipeline = TestDetectionPipeline(pipeline_args, tiktoken.encoding_for_model("gpt-4"))
        final_status = pipeline.run()
        
        # è®°å½•ç»“æœ
        result['iterations_completed'] = pipeline.current_iteration
        result['status'] = final_status or 'failed'
        
        if final_status == 'success':
            logger.info(f"âœ… [{project_index}/{total_projects}] Successfully processed: {project_name}")
            result['copied_to_runnable'] = True
        elif final_status == 'failed':
            logger.info(f"âŒ [{project_index}/{total_projects}] Failed: {project_name}")
        elif final_status == 'interrupted':
            logger.info(f"â¸ï¸ [{project_index}/{total_projects}] Interrupted: {project_name}")
        else:
            logger.warning(f"â“ [{project_index}/{total_projects}] Unknown status: {project_name}")

    except KeyboardInterrupt:
        logger.info(f"âŒ [{project_index}/{total_projects}] Interrupted by user: {project_name}")
        result['error_message'] = "Interrupted by user"
        result['status'] = 'interrupted'
        
    except Exception as e:
        error_msg = f"Failed to process project {project_name}: {str(e)}"
        logger.error(f"âŒ [{project_index}/{total_projects}] {error_msg}")
        logger.error(f"Full traceback: {traceback.format_exc()}")
        result['error_message'] = str(e)
        result['status'] = 'failed'
    
    finally:
        end_time = datetime.now()
        result['end_time'] = end_time.isoformat()
        result['execution_time_seconds'] = (end_time - start_time).total_seconds()
        
        logger.info(f"ğŸ“‹ [{project_index}/{total_projects}] Completed {project_name}: {result['status']} ({result['execution_time_seconds']:.1f}s)")
    
    return result


class MultiProcessBatchRunner:
    def __init__(self, args):
        self.args = args
        self.language = "Matlab"  # å›ºå®šä¸ºMatlab
        self.num_processes = args.num_processes
        
        # ä½¿ç”¨ä¸åŸè„šæœ¬ç›¸åŒçš„ç›®å½•ç»“æ„
        self.primary_filter_base = Path("primary_filter_v3_1").resolve()
        self.runnable_filter_base = Path("runnable_filter_1").resolve()
        
        # éªŒè¯ç›®å½•å­˜åœ¨
        if not self.primary_filter_base.exists():
            raise FileNotFoundError(f"Primary filter directory does not exist: {self.primary_filter_base}")
        
        # åˆ›å»ºrunnable_filterç›®å½•
        self.runnable_filter_base.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ‰¹å¤„ç†è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.batch_output_dir = Path("multiprocess_batch_logs") / f"matlab_batch_{timestamp}"
        self.batch_output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ‰¹å¤„ç†æ‘˜è¦
        self.batch_summary = {
            'start_time': datetime.now().isoformat(),
            'language': self.language,
            'num_processes': self.num_processes,
            'total_projects': 0,
            'successful_projects': 0,
            'failed_projects': 0,
            'interrupted_projects': 0,
            'results': []
        }
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(0)

    def discover_matlab_projects(self) -> List[Path]:
        """å‘ç°æ‰€æœ‰Matlabé¡¹ç›®"""
        matlab_dir = self.primary_filter_base / self.language
        
        if not matlab_dir.exists():
            raise FileNotFoundError(f"Matlab directory does not exist: {matlab_dir}")
        
        projects = []
        try:
            for item in matlab_dir.iterdir():
                if item.is_dir() and not item.name.startswith('.'):
                    projects.append(item)
            
            # æŒ‰åç§°æ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
            projects.sort(key=lambda x: x.name.lower())
            
            self.logger.info(f"Found {len(projects)} Matlab projects")
            return projects
            
        except Exception as e:
            self.logger.error(f"Error discovering Matlab projects: {e}")
            return []

    def split_projects_into_chunks(self, projects: List[Path]) -> List[List[Tuple]]:
        """å°†é¡¹ç›®åˆ†æˆå¤šä¸ªè¿›ç¨‹å—"""
        total_projects = len(projects)
        chunk_size = (total_projects + self.num_processes - 1) // self.num_processes
        
        chunks = []
        for i in range(self.num_processes):
            start_idx = i * chunk_size
            end_idx = min(start_idx + chunk_size, total_projects)
            
            if start_idx >= total_projects:
                break
                
            chunk_projects = projects[start_idx:end_idx]
            
            # ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºå‚æ•°å…ƒç»„
            chunk_args = []
            for j, project_path in enumerate(chunk_projects):
                project_index = start_idx + j + 1  # å…¨å±€ç´¢å¼•
                args_tuple = (
                    project_path,
                    self.language,
                    self.args.model_name,
                    self.args.max_iterations,
                    self.args.timeout_per_command,
                    self.args.verbose,
                    i,  # process_id
                    project_index,  # project_index
                    total_projects  # total_projects
                )
                chunk_args.append(args_tuple)
            
            chunks.append(chunk_args)
            self.logger.info(f"Process {i}: {len(chunk_args)} projects (indices {start_idx+1}-{end_idx})")
        
        return chunks

    def run_process_chunk(self, chunk_args: List[Tuple]) -> List[Dict]:
        """è¿è¡Œä¸€ä¸ªè¿›ç¨‹å—ä¸­çš„æ‰€æœ‰é¡¹ç›®"""
        if not chunk_args:
            return []
        
        process_id = chunk_args[0][6]  # ä»å‚æ•°å…ƒç»„ä¸­è·å–process_id
        logger = setup_logging(process_id)
        
        logger.info(f"ğŸš€ Process {process_id} starting with {len(chunk_args)} projects")
        
        results = []
        for args_tuple in chunk_args:
            try:
                result = run_single_project(args_tuple)
                results.append(result)
                
                # åœ¨è¿›ç¨‹é—´æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…è¿‡åº¦ç«äº‰èµ„æº
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"Error processing project in process {process_id}: {e}")
                # åˆ›å»ºé”™è¯¯ç»“æœ
                project_path = args_tuple[0]
                error_result = {
                    'process_id': process_id,
                    'project_index': args_tuple[7],
                    'project_name': project_path.name,
                    'language': self.language,
                    'project_path': str(project_path),
                    'start_time': datetime.now().isoformat(),
                    'end_time': datetime.now().isoformat(),
                    'status': 'failed',
                    'error_message': str(e),
                    'iterations_completed': 0,
                    'execution_time_seconds': 0,
                    'copied_to_runnable': False
                }
                results.append(error_result)
        
        logger.info(f"ğŸ Process {process_id} completed {len(results)} projects")
        return results

    def save_batch_summary(self):
        """ä¿å­˜æ‰¹å¤„ç†æ‘˜è¦"""
        self.batch_summary['end_time'] = datetime.now().isoformat()
        
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        if 'start_time' in self.batch_summary:
            start = datetime.fromisoformat(self.batch_summary['start_time'])
            end = datetime.fromisoformat(self.batch_summary['end_time'])
            self.batch_summary['total_execution_time_seconds'] = (end - start).total_seconds()
        
        # ä¿å­˜JSONæ‘˜è¦
        summary_file = self.batch_output_dir / "batch_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(self.batch_summary, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜äººç±»å¯è¯»çš„æ‘˜è¦
        summary_text_file = self.batch_output_dir / "batch_summary.txt"
        with open(summary_text_file, 'w', encoding='utf-8') as f:
            f.write("Multi-Process Matlab Batch Test Detection Summary\n")
            f.write("=" * 60 + "\n")
            f.write(f"Start Time: {self.batch_summary['start_time']}\n")
            f.write(f"End Time: {self.batch_summary['end_time']}\n")
            f.write(f"Total Execution Time: {self.batch_summary.get('total_execution_time_seconds', 0):.2f} seconds\n")
            f.write(f"Language: {self.batch_summary['language']}\n")
            f.write(f"Number of Processes: {self.batch_summary['num_processes']}\n")
            f.write("\n")
            f.write(f"ğŸ“Š RESULTS SUMMARY:\n")
            f.write(f"Total Projects: {self.batch_summary['total_projects']}\n")
            f.write(f"âœ… Successful Projects: {self.batch_summary['successful_projects']}\n")
            f.write(f"âŒ Failed Projects: {self.batch_summary['failed_projects']}\n")
            f.write(f"â¸ï¸ Interrupted Projects: {self.batch_summary['interrupted_projects']}\n")
            
            if self.batch_summary['total_projects'] > 0:
                success_rate = (self.batch_summary['successful_projects'] / self.batch_summary['total_projects']) * 100
                f.write(f"\nğŸ“ˆ Success Rate: {success_rate:.2f}%\n")
            
            # æŒ‰è¿›ç¨‹ç»Ÿè®¡
            f.write(f"\nğŸ“Š RESULTS BY PROCESS:\n")
            f.write("-" * 30 + "\n")
            
            process_stats = {}
            for result in self.batch_summary['results']:
                pid = result['process_id']
                if pid not in process_stats:
                    process_stats[pid] = {'total': 0, 'success': 0, 'failed': 0, 'interrupted': 0}
                
                process_stats[pid]['total'] += 1
                if result['status'] == 'success':
                    process_stats[pid]['success'] += 1
                elif result['status'] == 'interrupted':
                    process_stats[pid]['interrupted'] += 1
                else:
                    process_stats[pid]['failed'] += 1
            
            for pid in sorted(process_stats.keys()):
                stats = process_stats[pid]
                f.write(f"Process {pid}:\n")
                f.write(f"  Total: {stats['total']}\n")
                f.write(f"  âœ… Success: {stats['success']}\n")
                f.write(f"  âŒ Failed: {stats['failed']}\n")
                f.write(f"  â¸ï¸ Interrupted: {stats['interrupted']}\n")
                if stats['total'] > 0:
                    success_rate = (stats['success'] / stats['total']) * 100
                    f.write(f"  Success Rate: {success_rate:.1f}%\n")
                f.write("\n")
            
            f.write(f"ğŸ“‹ DETAILED RESULTS:\n")
            f.write("-" * 30 + "\n")
            
            # æŒ‰é¡¹ç›®ç´¢å¼•æ’åº
            sorted_results = sorted(self.batch_summary['results'], key=lambda x: x['project_index'])
            
            for result in sorted_results:
                status_emoji = {
                    'success': 'âœ…',
                    'failed': 'âŒ',
                    'interrupted': 'â¸ï¸'
                }.get(result['status'], 'â“')
                
                f.write(f"[{result['project_index']}] {result['project_name']} (Process {result['process_id']})\n")
                f.write(f"  Status: {status_emoji} {result['status'].upper()}\n")
                f.write(f"  Iterations: {result['iterations_completed']}\n")
                f.write(f"  Execution Time: {result['execution_time_seconds']:.2f}s\n")
                if result['copied_to_runnable']:
                    f.write(f"  ğŸ“ Copied to runnable_filter: Yes\n")
                if result['error_message']:
                    f.write(f"  Error: {result['error_message']}\n")
                f.write("\n")
        
        self.logger.info(f"ğŸ“‹ Batch summary saved to: {summary_file}")

    def run(self):
        """è¿è¡Œå¤šè¿›ç¨‹æ‰¹å¤„ç†ç®¡é“"""
        self.logger.info(f"ğŸš€ Starting Multi-Process Matlab Batch Pipeline")
        self.logger.info(f"Number of processes: {self.num_processes}")
        self.logger.info(f"Primary filter: {self.primary_filter_base}")
        self.logger.info(f"Runnable filter: {self.runnable_filter_base}")
        self.logger.info(f"Batch output directory: {self.batch_output_dir}")
        self.logger.info("-" * 60)
        
        try:
            # å‘ç°æ‰€æœ‰Matlabé¡¹ç›®
            projects = self.discover_matlab_projects()
            if not projects:
                self.logger.error("No Matlab projects found!")
                return False
            
            self.batch_summary['total_projects'] = len(projects)
            self.logger.info(f"Found {len(projects)} Matlab projects to process")
            
            # å°†é¡¹ç›®åˆ†æˆå¤šä¸ªè¿›ç¨‹å—
            process_chunks = self.split_projects_into_chunks(projects)
            self.logger.info(f"Split into {len(process_chunks)} process chunks")
            
            # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
            all_results = []
            
            if self.num_processes == 1:
                # å•è¿›ç¨‹æ¨¡å¼
                self.logger.info("Running in single-process mode")
                for chunk_args in process_chunks:
                    chunk_results = self.run_process_chunk(chunk_args)
                    all_results.extend(chunk_results)
            else:
                # å¤šè¿›ç¨‹æ¨¡å¼
                self.logger.info(f"Running in multi-process mode with {self.num_processes} processes")
                
                with Pool(processes=self.num_processes) as pool:
                    # æäº¤æ‰€æœ‰è¿›ç¨‹å—
                    futures = []
                    for chunk_args in process_chunks:
                        future = pool.apply_async(self.run_process_chunk, (chunk_args,))
                        futures.append(future)
                    
                    # æ”¶é›†ç»“æœ
                    for i, future in enumerate(futures):
                        try:
                            chunk_results = future.get(timeout=3600)  # 1å°æ—¶è¶…æ—¶
                            all_results.extend(chunk_results)
                            self.logger.info(f"âœ… Process chunk {i} completed with {len(chunk_results)} results")
                        except Exception as e:
                            self.logger.error(f"âŒ Process chunk {i} failed: {e}")
            
            # æ±‡æ€»ç»“æœ
            self.batch_summary['results'] = all_results
            
            for result in all_results:
                if result['status'] == 'success':
                    self.batch_summary['successful_projects'] += 1
                elif result['status'] == 'interrupted':
                    self.batch_summary['interrupted_projects'] += 1
                else:
                    self.batch_summary['failed_projects'] += 1
            
            self.logger.info(f"âœ… All processes completed!")
            
        except KeyboardInterrupt:
            self.logger.info("ğŸ›‘ Batch processing interrupted by user")
            
        except Exception as e:
            self.logger.error(f"âŒ Batch processing failed: {e}")
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
        
        finally:
            # ä¿å­˜æ‘˜è¦
            self.save_batch_summary()
            
            # æ‰“å°æœ€ç»ˆæ‘˜è¦
            self.logger.info("ğŸ Multi-Process Matlab Batch Complete!")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š FINAL RESULTS:")
            self.logger.info(f"Total Projects: {self.batch_summary['total_projects']}")
            self.logger.info(f"âœ… Successful: {self.batch_summary['successful_projects']}")
            self.logger.info(f"âŒ Failed: {self.batch_summary['failed_projects']}")
            self.logger.info(f"â¸ï¸ Interrupted: {self.batch_summary['interrupted_projects']}")
            
            if self.batch_summary['total_projects'] > 0:
                success_rate = (self.batch_summary['successful_projects'] / self.batch_summary['total_projects']) * 100
                self.logger.info(f"ğŸ¯ Success Rate: {success_rate:.2f}%")
            
            self.logger.info(f"ğŸ“ Runnable projects saved to: {self.runnable_filter_base}")
            
            return self.batch_summary['successful_projects'] > 0


def main():
    parser = argparse.ArgumentParser(description="Multi-Process Batch Test Detection for Matlab Projects")
    parser.add_argument('--model_name', type=str, default="claude-3-7-sonnet-20250219",
                       help="Model name to use (default: claude-3-7-sonnet-20250219)")
    parser.add_argument('--num_processes', type=int, default=10,
                       help="Number of parallel processes (default: 10)")
    parser.add_argument('--max_iterations', type=int, default=10,
                       help="Maximum iterations per project (default: 10)")
    parser.add_argument('--timeout_per_command', type=int, default=20,
                       help="Timeout per command in seconds (default: 20)")
    parser.add_argument('--verbose', action='store_true',
                       help="Enable verbose logging")
    args = parser.parse_args()
    
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    if hasattr(multiprocessing, 'set_start_method'):
        try:
            multiprocessing.set_start_method('spawn', force=True)
        except RuntimeError:
            pass  # å·²ç»è®¾ç½®è¿‡äº†
    
    print(f"ğŸš€ Starting Multi-Process Matlab Batch Pipeline")
    print(f"Number of processes: {args.num_processes}")
    print(f"Expected ~1885 Matlab projects")
    print(f"Model: {args.model_name}")
    print(f"Max iterations per project: {args.max_iterations}")
    print(f"Command timeout: {args.timeout_per_command}s")
    print("-" * 60)
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œæ‰¹å¤„ç†ç®¡é“
        batch_runner = MultiProcessBatchRunner(args)
        has_successful_projects = batch_runner.run()
        
        # é€‚å½“çš„é€€å‡ºç 
        if has_successful_projects:
            print("âœ… Batch completed with successful projects")
            sys.exit(0)
        else:
            print("âš ï¸ Batch completed but no projects were successful")
            sys.exit(1)
            
    except Exception as e:
        print(f"ğŸ’¥ Batch pipeline failed: {e}")
        print(f"Full traceback: {traceback.format_exc()}")
        sys.exit(2)


if __name__ == "__main__":
    main()